{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_point1:  135 num_point2:  408 layerdrop_rate:  0.0 used_folds [2]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import multiprocessing as mp\n",
    "from config import *\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = col\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    return data.values.astype(np.float32)\n",
    "import time\n",
    "def read_dict(file_path):\n",
    "    path = os.path.expanduser(file_path)\n",
    "    with open(path, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    return dic\n",
    "class TrainingLogger:\n",
    "    def __init__(self, log_file_path, headers, sep='    | ',print_to_stdout=True,p=3,resume=False):\n",
    "        self.log_file_path = log_file_path\n",
    "        self.headers = headers\n",
    "        self.sep = sep\n",
    "        self.print_to_stdout = print_to_stdout\n",
    "        self.col_widths = [len(h) for h in headers]\n",
    "        self.start_time = time.time()\n",
    "        self.elapsed_time = 0\n",
    "        self.elapsed_time_offset=0\n",
    "        self.resume = resume\n",
    "        self.first_log = True\n",
    "        self.p = p\n",
    "    def _write_headers(self):\n",
    "        with open(self.log_file_path, 'w') as f:\n",
    "            header_str = self.sep.join(self.headers) + ' | Time Elapsed'\n",
    "            f.write(header_str + '\\n')\n",
    "        if self.print_to_stdout:\n",
    "            print(header_str)\n",
    "    def _get_padded_strings(self, data):\n",
    "        padded_strings = []\n",
    "        for i, value in enumerate(data):\n",
    "            if isinstance(value, str):\n",
    "                padded_string = value.ljust(self.col_widths[i])\n",
    "            else:\n",
    "                padded_string = str(round(value,self.p)).ljust(self.col_widths[i])\n",
    "            padded_strings.append(padded_string)\n",
    "        return padded_strings\n",
    "    def log(self, *args):\n",
    "        if self.first_log:\n",
    "            self._write_headers()\n",
    "            self.first_log = False\n",
    "        assert len(args) == len(self.headers), f'Length of arguments should be {len(self.headers)}.'\n",
    "        with open(self.log_file_path, 'a') as f:\n",
    "\n",
    "            elapsed_time = time.time() - self.start_time+self.elapsed_time_offset\n",
    "            self.elapsed_time = elapsed_time\n",
    "            time_str = time.strftime('%H:%M:%S', time.gmtime(elapsed_time))\n",
    "            data_strings = self._get_padded_strings(args) + [time_str]\n",
    "            log_str = self.sep.join(data_strings)\n",
    "            f.write(log_str + '\\n')\n",
    "        if self.print_to_stdout:\n",
    "            print(log_str)\n",
    "    def info(self,message):\n",
    "        #记录寻常的日志信息\n",
    "        with open(self.log_file_path, 'a') as f:\n",
    "            f.write(message + '\\n')\n",
    "        if self.print_to_stdout:\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    #the following line gives ~10% speedup\n",
    "    #but may lead to some stochasticity in the results \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "def resume_fn(fname,model,swa_model,optim,scheduler):\n",
    "    if os.path.exists(fname): \n",
    "        data = torch.load(fname, map_location=\"cpu\")\n",
    "        torch.set_rng_state(data['torch_rng_state'])\n",
    "        torch.cuda.set_rng_state(data['cuda_rng_state'])\n",
    "        np.random.set_state(data['numpy_rng_state'])\n",
    "        random.setstate(data['random_rng_state'])\n",
    "        model.load_state_dict(data['state_dict'], strict=True)\n",
    "        swa_model.load_state_dict(data['swa_model'])\n",
    "        optim.load_state_dict(data['optimizer'])\n",
    "        scheduler.load_state_dict(data['scheduler'])\n",
    "        start_epoch = data['epoch'] + 1\n",
    "        val_score = data['val_score']\n",
    "        best_score = data['best_score']\n",
    "        patience = data['patience']\n",
    "        elapsed_time_offset = data['elapsed_time_offset']\n",
    "        print(f\" resume from {fname} \\n at epoch {start_epoch-1} \\n with val_score {val_score} best_score {best_score} and elapsed_time_offset {elapsed_time_offset}\")\n",
    "        return model,swa_model, optim, scheduler, start_epoch, best_score, patience,elapsed_time_offset\n",
    "    else:\n",
    "        print('no checkpoint found at %s', fname)\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.encoder = timm.create_model('tf_efficientnet_b0_ns', pretrained=True,in_chans=2*len(pointset1),num_classes=250,drop_rate=dropout)\n",
    "#     def forward(self, x):\n",
    "#         x = torch.cat([x[:,:,:,:,0],x[:,:,:,:,1]],dim=1)\n",
    "#         x = self.encoder(x)\n",
    "#         return x\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh(np.sqrt(2/np.pi)*(x+0.044715*torch.pow(x,3))))\n",
    "act={ \"quickgelu\":QuickGELU,\"gelu\":GELU,\"relu\":nn.ReLU}\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model,h):\n",
    "        super().__init__()\n",
    "        d_k = d_model // h//1\n",
    "        d_v = d_model // h\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "    def forward(self, queries, keys, values, attention_mask=None):\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "        q = self.fc_q(queries).reshape(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).reshape(keys.shape[0], nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).reshape(values.shape[0], nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "        att = torch.matmul(q, k) / (self.d_k**0.5)  # (b_s, h, nq, nk)\n",
    "        if attention_mask is not None:\n",
    "            att = att+attention_mask*torch.finfo(torch.float32).min\n",
    "        att = torch.softmax(att, -1)\n",
    "        att = F.dropout(att, p=dropout, training=self.training)\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).reshape(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)\n",
    "        return out\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_model)\n",
    "        self.fc2 = nn.Linear(d_model, d_model)\n",
    "        self.relu = act[act_name]()\n",
    "    def forward(self, input):\n",
    "        out = self.fc1(input)\n",
    "        out = self.relu(out)\n",
    "        out = F.dropout(out, p=dropout, training=self.training)\n",
    "        out = self.fc2(out)\n",
    "        out = F.dropout(out, p=dropout, training=self.training)\n",
    "        return out\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "        d_model,\n",
    "        num_head,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn  = MultiHeadAttention(d_model, num_head)\n",
    "        # self.ffn   = PositionWiseFeedForward(d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, x_mask=None):\n",
    "        out = self.norm1(x)\n",
    "        out = x + F.dropout(self.attn(out,out,out, x_mask), p=dropout, training=self.training)\n",
    "        out = self.norm2(out)\n",
    "        return out\n",
    "#########################################################################\n",
    " \n",
    "#########################################################################\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self,hidden_state, attention_mask):\n",
    "\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(hidden_state.size())\n",
    "        )\n",
    "        mean_embeddings = torch.sum(hidden_state * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9)\n",
    "        return mean_embeddings\n",
    "\n",
    "num_main_feats = len(LHAND)*2*2*n+num_point1*2\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Sequential(nn.Linear(input_dim[0]//2,hidden_dim),nn.LayerNorm(hidden_dim),act[act_name](),nn.Dropout(dropout)\n",
    "                                )\n",
    "        self.fc2 = nn.Sequential(nn.Linear(input_dim[0]//2,hidden_dim),nn.LayerNorm(hidden_dim),act[act_name](),nn.Dropout(dropout)\n",
    "                                )\n",
    "        self.fc3 = nn.Sequential(nn.Linear(input_dim[1]//2,hidden_dim),nn.LayerNorm(hidden_dim),act[act_name](),nn.Dropout(dropout)\n",
    "                                )\n",
    "        self.fc4 = nn.Sequential(nn.Linear(input_dim[1]//2,hidden_dim),nn.LayerNorm(hidden_dim),act[act_name](),nn.Dropout(dropout)\n",
    "                                )\n",
    "        self.fc5 = nn.Sequential(nn.Linear(input_dim[2]//2,hidden_dim),nn.LayerNorm(hidden_dim),act[act_name](),nn.Dropout(dropout)\n",
    "                                )\n",
    "        self.fc6 = nn.Sequential(nn.Linear(input_dim[2]//2,hidden_dim),nn.LayerNorm(hidden_dim),act[act_name](),nn.Dropout(dropout)\n",
    "                                )\n",
    "        self.fc = nn.Sequential(nn.Linear(6*hidden_dim,embed_dim),nn.LayerNorm(embed_dim),act[act_name](),nn.Dropout(dropout),\n",
    "                                # nn.Linear(embed_dim,embed_dim),nn.LayerNorm(embed_dim),act[act_name](),nn.Dropout(dropout)\n",
    "                                )\n",
    "    def forward(self,inputs):\n",
    "        x1 = inputs[:,:,:input_dim[0]]\n",
    "        x2 = inputs[:,:,input_dim[0]:input_dim[0]+input_dim[1]]\n",
    "        x3 = inputs[:,:,input_dim[0]+input_dim[1]:]\n",
    "        x1 = torch.cat([self.fc1(x1[...,:input_dim[0]//2]),self.fc2(x1[...,input_dim[0]//2:])],dim=-1)\n",
    "        x2 = torch.cat([self.fc3(x2[...,:input_dim[1]//2]),self.fc4(x2[...,input_dim[1]//2:])],dim=-1)\n",
    "        x3 = torch.cat([self.fc5(x3[...,:input_dim[2]//2]),self.fc6(x3[...,input_dim[2]//2:])],dim=-1)\n",
    "        x = torch.cat([x1,x2,x3],dim=-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_class=250):\n",
    "        super().__init__()\n",
    "        self.layerdrop=nn.Parameter(torch.tensor(layerdrop), requires_grad=False)\n",
    "        self.layerdrop_decay=None\n",
    "        \n",
    "        self.x_embed = TransformerEmbedding()\n",
    "        self.pos_embed = nn.Parameter(torch.zeros((max_length, embed_dim)), requires_grad=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.encoder=TransformerBlock(embed_dim, num_head)\n",
    "        self.logit = nn.Sequential( nn.Linear(embed_dim, num_class))\n",
    "        self._init_weights()    \n",
    "    def _init_weights(self):\n",
    "        #init weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    def forward(self, x,x_mask):\n",
    "      \n",
    "        B,L= x.shape[:2]\n",
    "        x = self.x_embed(x)\n",
    "        x = x + self.pos_embed[:L].unsqueeze(0)\n",
    "        x = self.norm(x)\n",
    "        x_mask = x_mask.unsqueeze(1).unsqueeze(2)\n",
    "        if   self.training and random.random() < self.layerdrop:\n",
    "            pass\n",
    "        else:\n",
    "            x = self.encoder(x,x_mask)\n",
    "        mean_pool = MeanPooling()(x,1-x_mask[:,0,0,:])\n",
    "        mean_pool  =F.dropout(mean_pool, p=dropout, training=self.training)\n",
    "        logit = self.logit(mean_pool)\n",
    "        # logit = logit.reshape(B, -1)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (x_embed): TransformerEmbedding(\n",
       "    (fc1): Sequential(\n",
       "      (0): Linear(in_features=135, out_features=128, bias=True)\n",
       "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): QuickGELU()\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (fc2): Sequential(\n",
       "      (0): Linear(in_features=135, out_features=128, bias=True)\n",
       "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): QuickGELU()\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (fc3): Sequential(\n",
       "      (0): Linear(in_features=672, out_features=128, bias=True)\n",
       "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): QuickGELU()\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (fc4): Sequential(\n",
       "      (0): Linear(in_features=672, out_features=128, bias=True)\n",
       "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): QuickGELU()\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (fc5): Sequential(\n",
       "      (0): Linear(in_features=882, out_features=128, bias=True)\n",
       "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): QuickGELU()\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (fc6): Sequential(\n",
       "      (0): Linear(in_features=882, out_features=128, bias=True)\n",
       "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): QuickGELU()\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): QuickGELU()\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder): TransformerBlock(\n",
       "    (attn): MultiHeadAttention(\n",
       "      (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (logit): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=250, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(Model().state_dict(), 'model.pth')\n",
    "Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'njkb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnjkb\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'njkb' is not defined"
     ]
    }
   ],
   "source": [
    "# njkb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# def pre_process(xyz):\n",
    "#     xyz = xyz - xyz[~torch.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common mean\n",
    "#     xyz = xyz / xyz[~torch.isnan(xyz)].std(0, keepdims=True)\n",
    "#     xyz[torch.isnan(xyz)] = 0\n",
    "#     xyz = center_crop(xyz, len(xyz))\n",
    "#     return xyz\n",
    "class aug():\n",
    "    def __init__(self,p=0.):\n",
    "        self.p = p\n",
    "        self.fd = 0.4\n",
    "    def __call__(self,sample):\n",
    "        sample = self.flip(sample)\n",
    "        sample = self.random_drop(sample)\n",
    "        return sample\n",
    "    def random_drop(self,sample):\n",
    "        if self.p>0:\n",
    "            indices=[]\n",
    "            while len(indices) ==0:\n",
    "                indices = (torch.rand(sample.shape[0]) >=self.fd).nonzero().squeeze(1)\n",
    "            sample = sample[indices]\n",
    "        return sample\n",
    "    def flip(self,sample):\n",
    "        if torch.rand(1) < self.p:\n",
    "            x = sample[:,:,0]\n",
    "            x_max = x[~torch.isnan(x)].max()\n",
    "            x_min = x[~torch.isnan(x)].min()\n",
    "            x = x_max - x + x_min\n",
    "            sample[:,:,0] = x\n",
    "            tmp = sample[:,LHAND]\n",
    "            sample[:,LHAND] = sample[:,RHAND]\n",
    "            sample[:,RHAND] = tmp\n",
    "        return sample\n",
    "def norm(x):\n",
    "    x_mean, x_std = x[~torch.isnan(x)].mean(0,keepdim=True), x[~torch.isnan(x)].std(0,keepdim=True)\n",
    "    x = (x - x_mean) / x_std\n",
    "    x[torch.isnan(x)] = 0\n",
    "    return x\n",
    "def norm_xy(x):\n",
    "    return torch.cat([norm(x[...,0:1]),norm(x[...,1:2])],dim=-1)\n",
    "def pre_process(x):\n",
    "    for i in range(x.shape[-1]):\n",
    "        x[:,:,i] = norm(x[:,:,i])\n",
    "    x = center_crop(x, len(x))\n",
    "    return x\n",
    "def center_crop(xyz, valid_len):#input shape (none,543,2)\n",
    "    if valid_len > max_length:\n",
    "        start = (valid_len - max_length) // 2\n",
    "        end = start + max_length\n",
    "        xyz = xyz[start:end]\n",
    "    return xyz\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self, df, augment=None):\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.length = len(self.df)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "\n",
    "        pq_file =f'/mnt/hdd1/wangjingqi/GD/{d.path}'\n",
    "        x = load_relevant_data_subset(pq_file)\n",
    "        \n",
    "        n_frames = int(len(x) / ROWS_PER_FRAME)\n",
    "        x = x.reshape(n_frames, ROWS_PER_FRAME, len(col))\n",
    "        # x = x[:,pointset1]\n",
    "        x = torch.from_numpy(x).float()\n",
    "        if self.augment is not None:\n",
    "            x = self.augment(x)\n",
    "        xy = norm_xy(x)[:,pointset1]\n",
    "        xy = center_crop(xy, len(xy))\n",
    "        x = x[:,pointset1]\n",
    "        x = center_crop(x, len(x))\n",
    "\n",
    "        lhand = x[:,len(LIP):len(LIP)+len(LHAND)]\n",
    "        rhand = x[:,len(LIP)+len(LHAND):]\n",
    "        relative_lhand =norm_xy (lhand.unsqueeze(1) - lhand.unsqueeze(2))\n",
    "        relative_rhand =norm_xy (rhand.unsqueeze(1) - rhand.unsqueeze(2))\n",
    "        x = x[:,len(LIP):]\n",
    "        x = x.permute(1,0,2)#shape (num_point1, max_length, 2)\n",
    "        relative_back = torch.zeros(x.shape[0],x.shape[1],n//2,2)\n",
    "        relative_front = torch.zeros(x.shape[0],x.shape[1],n//2,2)\n",
    "        for i in range(n//2):\n",
    "            off =x[:,i+1:]-x[:,:-i-1]\n",
    "            relative_back[:,i+1:,i] = off\n",
    "            relative_front[:,:-i-1,i] = -off\n",
    "        relative = torch.cat([relative_back,relative_front],-2)#shape (len(LHAND)*2, max_length, n, 2)\n",
    "        relative_xy = norm_xy(relative).permute(1,0,2,3)\n",
    "        # relative_xy = torch.cat([norm_xy(relative[:len(LHAND)]),norm_xy(relative[len(LHAND):])],dim=0).permute(1,0,2,3)\n",
    "        x = xy[...,0:1].flatten(-2)#shape (max_length, num_point1*2)\n",
    "        y = xy[...,1:2].flatten(-2)\n",
    "        relative_x = relative_xy[...,0:1].flatten(-3)#shape (max_length,len(LHAND)*2*n*2)\n",
    "        relative_y = relative_xy[...,1:2].flatten(-3)\n",
    "        relative_lhand_x =  relative_lhand[...,0:1].flatten(-3)#shape (max_length,len(LHAND)*len(LHAND)*2)\n",
    "        relative_lhand_y =  relative_lhand[...,1:2].flatten(-3)\n",
    "        relative_rhand_x =  relative_rhand[...,0:1].flatten(-3)#shape (max_length,len(LHAND)*len(LHAND)*2)\n",
    "        relative_rhand_y =  relative_rhand[...,1:2].flatten(-3)\n",
    "        x = torch.cat([x,y,relative_x,relative_y,relative_lhand_x,relative_rhand_x,relative_lhand_y,relative_rhand_y],-1)\n",
    "        sample ={\"x\":x,\"label\":d.label}\n",
    "        return sample\n",
    "def pack_seq(\n",
    "    seq,\n",
    "):\n",
    "    max_len = max([ s.shape[0] for s in seq])\n",
    "    batch_size = len(seq)\n",
    "    x = torch.zeros(batch_size,max_len,seq[0].shape[-1])\n",
    "    x_mask = torch.zeros((batch_size, max_len))\n",
    "    for i in range(batch_size):\n",
    "        x[i,:seq[i].shape[0]] = seq[i]\n",
    "        x_mask[i, seq[i].shape[0]:] = 1\n",
    "    \n",
    "    return x, x_mask\n",
    "\n",
    "def null_collate(batch):\n",
    "    d = {}\n",
    "    key = batch[0].keys()\n",
    "    for k in key:\n",
    "        d[k] = [b[k] for b in batch]\n",
    "    d['label'] = torch.LongTensor(d['label'])\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "DEBUG = 0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def loss_fn(preds, labels):\n",
    "    loss = nn.CrossEntropyLoss(label_smoothing=ls)(preds, labels)\n",
    "    return loss\n",
    "def compute_score(preds, labels):\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    predict = np.argsort(-preds,-1)\n",
    "    correct = predict==labels.reshape(labels.shape[0],1)\n",
    "    topk = correct.cumsum(-1).mean(0)[:5]\n",
    "    return topk\n",
    "\n",
    "def train_fn(model, train_loader, optim, scaler,device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_score = 0\n",
    "    with tqdm(desc='train', total=len(train_loader)) as pbar:\n",
    "        for it,batch in enumerate(train_loader):\n",
    "            batch[\"x\"],batch[\"mask\"]=pack_seq(batch[\"x\"])\n",
    "            batch['x'] = batch['x'].to(device[0])\n",
    "            batch['mask'] = batch['mask'].to(device[0])\n",
    "            batch['label'] = batch['label'].to(device[0])\n",
    "            optim.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled = True):\n",
    "                preds = model(batch['x'],batch['mask'])\n",
    "                if preds.shape[0]!=batch[\"label\"].shape[0]:\n",
    "                    batch[\"label\"]=torch.cat([batch[\"label\"],batch[\"label\"]],0)\n",
    "                loss = loss_fn(preds, batch['label'])\n",
    "                score = compute_score(preds, batch['label'])\n",
    "                train_loss += loss.item()\n",
    "                train_score += score\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "            pbar.set_postfix_str(f'loss={train_loss/ (it + 1):.4f}')\n",
    "            pbar.update()\n",
    "            if DEBUG:\n",
    "                break\n",
    "    train_loss /= len(train_loader)\n",
    "    train_score /= len(train_loader)\n",
    "    return train_loss, train_score\n",
    "def validate_fn(model, valid_loader,device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_score = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(desc='valid', total=len(valid_loader)) as pbar:\n",
    "            for it,batch in enumerate(valid_loader):\n",
    "                batch[\"x\"],batch[\"mask\"]=pack_seq(batch[\"x\"])\n",
    "                batch['x'] = batch['x'].to(device[0])\n",
    "                batch['mask'] = batch['mask'].to(device[0])\n",
    "                batch['label'] = batch['label'].to(device[0])\n",
    "                preds = model(batch['x'],batch['mask'])\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(batch['label'])\n",
    "                pbar.update()\n",
    "                if DEBUG:\n",
    "                    break\n",
    "            all_preds = torch.cat(all_preds,0)\n",
    "            all_labels = torch.cat(all_labels,0)\n",
    "            val_loss = loss_fn(all_preds, all_labels)\n",
    "            val_score = compute_score(all_preds, all_labels)\n",
    "    return val_loss.item(), val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import gc\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "def train_one_fold(fold,exp_name,model_path,train_loader,valid_loader,model,optim,epochs,resume_last,resume_best,device,headers):\n",
    "    best_score = .0\n",
    "    patience = 0\n",
    "    start_epoch = 0\n",
    "    best_epoch = 0\n",
    "    exp_name = f\"{exp_name}_fold{fold}\"\n",
    "    log = TrainingLogger(f'{model_path}/log{exp_name}.txt', headers= headers)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_start = 40\n",
    "    scheduler = SWALR(optim, swa_lr=1e-4,anneal_epochs=1,anneal_strategy=\"linear\")\n",
    "    if resume_last or resume_best:\n",
    "        if resume_last:\n",
    "            fname = os.path.join(model_path,\"last\",f\"{exp_name}_last.pth\")\n",
    "        else:\n",
    "            fname = os.path.join(model_path, \"best\", f\"{exp_name}_best.pth\")\n",
    "        if os.path.exists(fname):\n",
    "            model,swa_model, optim, scheduler, start_epoch, best_score, patience,elapsed_time_offset = resume_fn(fname,model,swa_model,optim,scheduler)\n",
    "            log.elapsed_time_offset =elapsed_time_offset\n",
    "            log.first_log = False\n",
    "        else:\n",
    "            print(\"Warning: resume file not found\")\n",
    "    print(f\"Training {exp_name} from epoch {start_epoch}\")\n",
    "    \n",
    "    model = nn.DataParallel(model, device_ids=device, output_device=device[0])\n",
    "    \n",
    "    for e in range(start_epoch, epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # Training\n",
    "        train_loss, train_score = train_fn(model, train_loader, optim, scaler,device)\n",
    "        model.module.layerdrop -=model.module.layerdrop_decay\n",
    "        print(f\"layerdrop:{model.module.layerdrop}\")\n",
    "        # Validation\n",
    "        if e > swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "            scheduler.step()\n",
    "            print(\"swa update\",scheduler.get_last_lr())\n",
    "            val_loss, val_score = validate_fn(swa_model, valid_loader,device)\n",
    "        else:\n",
    "            val_loss, val_score = validate_fn(model, valid_loader,device)\n",
    "        train_top0, train_top1, train_top2, train_top3, train_top4 = train_score[0], train_score[1], train_score[2], train_score[3], train_score[4]\n",
    "        val_top0, val_top1, val_top2, val_top3, val_top4 = val_score[0], val_score[1], val_score[2], val_score[3], val_score[4]\n",
    "        log.log(e, train_loss, train_top0, val_loss, val_top0, val_top1, val_top2, val_top3, val_top4)\n",
    "        # log.log(e, train_loss, train_top0, val_loss, val_top0, val_top1, val_top2, val_top3, val_top4,str(scheduler.get_lr()[0]))\n",
    "        # Prepare for next epoch\n",
    "        \n",
    "        best=False\n",
    "        if val_top0 >= best_score:\n",
    "            best_score = val_top0\n",
    "            best_epoch = e\n",
    "            log.info(f\"Better score {best_score:.4f} is found at epoch {e}\")\n",
    "            patience = 0\n",
    "            best = True\n",
    "        else:\n",
    "            patience += 1\n",
    "        torch.save({\n",
    "            'torch_rng_state': torch.get_rng_state(),\n",
    "            'cuda_rng_state': torch.cuda.get_rng_state(),\n",
    "            'numpy_rng_state': np.random.get_state(),\n",
    "            'random_rng_state': random.getstate(),\n",
    "            'epoch': e,\n",
    "            \"fold\": fold,\n",
    "            \"exp_name\":exp_name,\n",
    "            'val_loss': val_loss,\n",
    "            'val_score': val_score,\n",
    "            \"elapsed_time_offset\": log.elapsed_time,\n",
    "            'state_dict': model.module.state_dict(),\n",
    "            \"swa_state_dict\":swa_model.module.state_dict(),\n",
    "            \"swa_model\":swa_model.state_dict(),\n",
    "            'optimizer': optim.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'patience': patience,\n",
    "            'best_score': best_score,\n",
    "        }, os.path.join(model_path, \"last\",f\"{exp_name}_last.pth\"))\n",
    "\n",
    "        if best:\n",
    "            shutil.copyfile(os.path.join(model_path, \"last\",f\"{exp_name}_last.pth\"), os.path.join(model_path, \"best\",f\"{exp_name}_best.pth\"))\n",
    "        # if patience >= patiences:\n",
    "        #     log.info(f\"Early stopping at epoch {e} with best score {best_score:.4f} at epoch {best_epoch}\")\n",
    "        #     break\n",
    "    log.info(f\"train done for fold {fold} with best score {best_score:.4f} at epoch {best_epoch} \\n\")\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "epochs = 200\n",
    "bs = 64\n",
    "patiences = 10\n",
    "n_fold = 5\n",
    "device = device\n",
    "resume_last = True\n",
    "resume_best = False\n",
    "exp_name = f\"n{n}_seed{seed}_{act_name}_fd{int(fd*10)}_p{int(p*10)}_ls{int(ls*10)}_pn{num_point1}_{num_block}_{embed_dim}_{hidden_dim}_{bs}_ld{int(layerdrop*10)}\"\n",
    "# exp_name = \"test\"\n",
    "model_path = f'/mnt/hdd1/wangjingqi/GD/asl/{exp_name}'\n",
    "\n",
    "HEADERS = ['epoch', 'train_loss',\"train_top0\", 'val_loss', 'val_top0', 'val_top1', 'val_top2', 'val_top3', 'val_top4']\n",
    "\n",
    "os.makedirs(os.path.join(model_path,\"last\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(model_path,\"best\"), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = pd.read_csv(\"/mnt/hdd1/wangjingqi/GD/train.csv\")\n",
    "label_index = read_dict(f\"/mnt/hdd1/wangjingqi/GD/sign_to_prediction_index_map.json\")\n",
    "index_label = dict([(label_index[key], key) for key in label_index])\n",
    "df[\"label\"] = df[\"sign\"].map(lambda sign: label_index[sign])\n",
    "groups = df[\"path\"].map(lambda x: x.split(\"/\")[1])\n",
    "sgkf = StratifiedGroupKFold(n_splits=n_fold, random_state=seed, shuffle=True)\n",
    "df[\"fold\"] = -1\n",
    "for i, (train_index, valid_index) in enumerate(sgkf.split(df[\"path\"],df[\"label\"], df[\"participant_id\"])):\n",
    "    df.loc[valid_index, \"fold\"] = i\n",
    "df.to_csv(f\"/mnt/hdd1/wangjingqi/GD/train_fold.csv\", index=False)\n",
    "# df = pd.read_csv(\"/mnt/hdd1/wangjingqi/GDtrain_prepared.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_best_score = {}\n",
    "def get_optimizer_params(model, lr):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"lr\" : lr\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"lr\" : lr\n",
    "        }\n",
    "    ]\n",
    "    return optimizer_grouped_parameters\n",
    "for fold in used_folds:\n",
    "    seed_everything(seed)\n",
    "    model =  Model().to(device[0])\n",
    "    \n",
    "    print(model)\n",
    "    if optim_type == \"adamw\":\n",
    "        optim = torch.optim.AdamW(params=get_optimizer_params(model,lr))\n",
    "    train_df = df[df.fold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.fold == fold].reset_index(drop=True)\n",
    "    train_dataset = SignDataset(train_df,aug(p))\n",
    "    valid_dataset = SignDataset(valid_df,None)\n",
    "    print(np.sum(input_dim),valid_dataset[0][\"x\"].shape[-1])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=nw, pin_memory=True, drop_last=True,collate_fn=null_collate)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=bs, shuffle=False, num_workers=nw, pin_memory=True, drop_last=False,collate_fn=null_collate)\n",
    "    model.layerdrop_decay=layerdrop/epochs\n",
    "    best_score_fold = train_one_fold(fold,exp_name,model_path,train_loader,valid_loader,model,optim,epochs,resume_last,resume_best,device,HEADERS)\n",
    "    all_best_score[f\"fold{fold}\"] = best_score_fold\n",
    "    resume_best = False\n",
    "    resume_last = False\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"best socre for all fold is \",all_best_score)\n",
    "with open(\"/mnt/hdd1/wangjingqi/GD/asl/log.txt\",\"a\") as f:\n",
    "    f.write(f\"{exp_name} with epoch {epochs} lr {lr} bs {bs}  seed {seed} max_length {max_length} embed_dim {embed_dim} num blocks {num_block} num_heads {num_head}  \\n for all fold is {all_best_score} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0025a32c0f829490bf273620ded1461447ddddb75e195ca26bb4a81cc062d47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
