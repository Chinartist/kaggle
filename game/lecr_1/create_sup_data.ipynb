{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================================\n",
    "# Libraries\n",
    "# =========================================================================================\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "import cupy as cp\n",
    "from cuml.metrics import pairwise_distances\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "# =========================================================================================\n",
    "# Data Loading\n",
    "# =========================================================================================\n",
    "import re\n",
    "import string\n",
    "\n",
    "def read_data(cfg):\n",
    "    content = pd.read_csv(\"/mnt/hdd1/wangjingqi/dataset/lecr/content.csv\")\n",
    "    topics = pd.read_csv(\"/mnt/hdd1/wangjingqi/dataset/lecr/topics.csv\")\n",
    "    correlations = pd.read_csv(\"/mnt/hdd1/wangjingqi/dataset/lecr/correlations.csv\")\n",
    "    \n",
    "    topics['title'].fillna(\"\", inplace = True)\n",
    "    content['title'].fillna(\"\", inplace = True)\n",
    "    topics['description'].fillna(\"\", inplace = True)\n",
    "    content['description'].fillna(\"\", inplace = True)\n",
    "    content['text'].fillna(\"\", inplace = True)\n",
    "\n",
    "\n",
    "    topics[\"Ti\"] = topics[\"title\"]\n",
    "    content[\"Ti\"] = content[\"title\"]\n",
    "    \n",
    "    topics[\"TiDe\"] = topics[\"title\"]+\" \"+topics[\"description\"]\n",
    "    content[\"TiDe\"] = content[\"title\"]+\" \"+content[\"description\"]\n",
    "\n",
    "    topics[\"TiDeTe\"] = topics[\"title\"]+\" \"+topics[\"description\"]\n",
    "    content[\"TiDeTe\"] = content[\"title\"]+\" \"+content[\"description\"]+\" \"+content[\"text\"]\n",
    "\n",
    "    topics['length'] = topics[cfg.uns_key].apply(lambda x: len(x))\n",
    "    content['length'] = content[cfg.uns_key].apply(lambda x: len(x))\n",
    "    \n",
    "    topics.sort_values('length', inplace = True)\n",
    "    content.sort_values('length', inplace = True)\n",
    "    # Drop cols\n",
    "    topics.drop(['title','description', 'channel', 'category', 'level', 'has_content', 'length'], axis = 1, inplace = True)\n",
    "    content.drop(['title','description', 'kind',  'text', 'copyright_holder', 'license', 'length'], axis = 1, inplace = True)\n",
    "    # Reset index\n",
    "    topics.reset_index(drop = True, inplace = True)\n",
    "    content.reset_index(drop = True, inplace = True)\n",
    "    print(' ')\n",
    "    print('-' * 50)\n",
    "    print(f\"topics.shape: {topics.shape}\")\n",
    "    print(f\"content.shape: {content.shape}\")\n",
    "    print(f\"correlations.shape: {correlations.shape}\")\n",
    "    return topics, content, correlations\n",
    "\n",
    "class LECRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,key):\n",
    "        self.inputs = df[key].values\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self,idx):\n",
    "        sample = self.inputs[idx]\n",
    "        return sample\n",
    "\n",
    "class collator():\n",
    "    def __init__(self,pretrained_path,max_len=None) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n",
    "        self.max_len = max_len\n",
    "    def __call__(self, data):\n",
    "        inputs = self.tokenize(list(data))\n",
    "        return inputs\n",
    "    def tokenize(self,texts):\n",
    "            return self.tokenizer(\n",
    "                texts,padding='longest',max_length=self.max_len,truncation=True,return_tensors=\"pt\",return_token_type_ids=False)\n",
    "# =========================================================================================\n",
    "# Unsupervised model\n",
    "# =========================================================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self,hidden_state, attention_mask):\n",
    "\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(hidden_state.size())\n",
    "        )\n",
    "        mean_embeddings = torch.sum(hidden_state * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9)\n",
    "        return mean_embeddings\n",
    "\n",
    "class UNSModel(nn.Module):\n",
    "    def __init__(self, pretrained_path):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(pretrained_path)\n",
    "        self.pool = MeanPooling()\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        return feature\n",
    "    \n",
    "# =========================================================================================\n",
    "# Get embeddings\n",
    "# =========================================================================================\n",
    "def get_embeddings(loader, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for step, inputs in enumerate(tqdm(loader)):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds\n",
    "\n",
    "# =========================================================================================\n",
    "# Get the amount of positive classes based on the total\n",
    "# =========================================================================================\n",
    "def get_pos_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    int_true = np.array([len(x[0] & x[1]) / len(x[0]) for x in zip(y_true, y_pred)])\n",
    "    return round(np.mean(int_true), 5)\n",
    "\n",
    "# =========================================================================================\n",
    "# Build our training set\n",
    "# =========================================================================================\n",
    "def build_training_set(topics, content, cfg):\n",
    "    # Create lists for training\n",
    "   \n",
    "    input_key = cfg.sup_key\n",
    "    topics_ids = []\n",
    "    content_ids = []\n",
    "    input1 = []\n",
    "    input2 = []\n",
    "    targets = []\n",
    "    topics_languages = []\n",
    "    content_languages = []\n",
    "    # Iterate over each topic\n",
    "    for k in tqdm(range(len(topics))):\n",
    "        row = topics.iloc[k]\n",
    "        topics_id = row['id']\n",
    "        topics_input = row[input_key]\n",
    "        topics_language = row['language']\n",
    "        predictions = row['predictions'].split(' ')\n",
    "        ground_truth = row['content_ids'].split(' ')\n",
    "        # predictions = list(set(predictions)|set(ground_truth))\n",
    "        for pred in predictions:\n",
    "            content_language = content.loc[pred, 'language']\n",
    "            content_input= content.loc[pred, input_key]\n",
    "            topics_ids.append(topics_id)\n",
    "            content_ids.append(pred)\n",
    "            input1.append(topics_input)\n",
    "            input2.append(content_input)\n",
    "            topics_languages.append(topics_language)\n",
    "            content_languages.append(content_language)\n",
    "            # If pred is in ground truth, 1 else 0\n",
    "            if pred in ground_truth:\n",
    "                targets.append(1)\n",
    "            else:\n",
    "                targets.append(0)\n",
    "    # Build training dataset\n",
    "    train = pd.DataFrame(\n",
    "        {'topics_ids': topics_ids, \n",
    "         'content_ids': content_ids, \n",
    "         'input1': input1, \n",
    "         'input2': input2, \n",
    "         'target': targets,\n",
    "         'topic_language': topics_languages, \n",
    "         'content_language': content_languages, }\n",
    "    )\n",
    "    # Release memory\n",
    "    del topics_ids, content_ids, input1, input2, targets\n",
    "    gc.collect()\n",
    "    return train\n",
    "    \n",
    "# =========================================================================================\n",
    "# Get neighbors\n",
    "# =========================================================================================\n",
    "def get_neighbors(topics, content, cfg):\n",
    "    # Create topics dataset\n",
    "    topics_dataset = LECRDataset(topics,cfg.uns_key)\n",
    "    # Create content dataset\n",
    "    content_dataset = LECRDataset(content,cfg.uns_key)\n",
    "    \n",
    "\n",
    "    collate_fn= collator(cfg.model_name, cfg.max_len)\n",
    "    # Create topics and content dataloaders\n",
    "    topics_loader = DataLoader(topics_dataset,batch_size = cfg.bs, shuffle = False, num_workers= cfg.nw, pin_memory=True,collate_fn =collate_fn,drop_last=False)\n",
    "    content_loader = DataLoader(content_dataset,batch_size = cfg.bs, shuffle = False, num_workers= cfg.nw, pin_memory=True,collate_fn =collate_fn,drop_last=False)\n",
    "    # Create unsupervised model to extract embeddings\n",
    "    model = UNSModel(cfg.model_name)\n",
    "    model.to(cfg.device)\n",
    "    model.float()\n",
    "    # Predict topics\n",
    "    \n",
    "\n",
    "\n",
    "    topics_preds = get_embeddings(topics_loader, model, cfg.device)\n",
    "    del topics_loader\n",
    "    gc.collect()\n",
    "    content_preds = get_embeddings(content_loader, model, cfg.device)\n",
    "    # Transfer predictions to gpu\n",
    "    topics_preds_gpu = cp.array(topics_preds)\n",
    "    content_preds_gpu = cp.array(content_preds)\n",
    "    # Release memory\n",
    "    torch.cuda.empty_cache()\n",
    "    del topics_dataset, content_dataset, content_loader, topics_preds, content_preds\n",
    "    gc.collect()\n",
    "    # KNN model\n",
    "    print(' ')\n",
    "    print('Training KNN model...')\n",
    "    neighbors_model = NearestNeighbors(n_neighbors = cfg.top_n, metric = 'cosine')\n",
    "    neighbors_model.fit(content_preds_gpu)\n",
    "    distances,indices = neighbors_model.kneighbors(topics_preds_gpu, return_distance = True)\n",
    "    distances = distances.get()\n",
    "    indices = indices.get()\n",
    "    predictions = []\n",
    "    for k in range(len(indices)):\n",
    "        pred = indices[k]\n",
    "        dis = distances[k]\n",
    "        p = []\n",
    "        for i in range(len(pred)):\n",
    "            if dis[i] < 1000:\n",
    "                p.append(content.loc[pred[i], 'id'])\n",
    "        if len(p)==0:\n",
    "            p = []\n",
    "            for i in range(len(pred)):\n",
    "                p.append(content.loc[pred[i], 'id'])\n",
    "        p = ' '.join(p)\n",
    "        predictions.append(p)\n",
    "    topics['predictions'] = predictions\n",
    "    # Release memory\n",
    "    del topics_preds_gpu, content_preds_gpu, neighbors_model, predictions, indices, model\n",
    "    gc.collect()\n",
    "    return topics, content \n",
    "\n",
    "# Read data\n",
    "# topics, content, correlations = read_data(CFG)\n",
    "# # Run nearest neighbors\n",
    "# topics, content = get_neighbors(topics, content, CFG)\n",
    "# # Merge with target and comput max positive score\n",
    "# topics = topics.merge(correlations, how = 'inner', left_on = ['id'], right_on = ['topic_id'])\n",
    "# pos_score = get_pos_score(topics['content_ids'], topics['predictions'])\n",
    "# print(f'Our max positive score is {pos_score}')\n",
    "# # We can delete correlations\n",
    "# del correlations\n",
    "# gc.collect()\n",
    "# # Set id as index for content\n",
    "# content.set_index('id', inplace = True)\n",
    "# # Build training set\n",
    "# train = build_training_set(topics, content, CFG)\n",
    "# print(f'Our training set has {len(train)} rows')\n",
    "# # Save train set to disk to train on another notebook\n",
    "# train.to_csv('train.csv', index = False)\n",
    "# train.head()\n",
    "def test(CFG,models):\n",
    "    scores = {}\n",
    "    for model in models:\n",
    "        id = model.split('/')[-1]\n",
    "        CFG.model_name = model\n",
    "        print(f'Running model {id}')\n",
    "        topics, content, correlations = read_data(CFG)\n",
    "        topics, content = get_neighbors(topics, content, CFG)\n",
    "        topics = topics.merge(correlations, how = 'inner', left_on = ['id'], right_on = ['topic_id'])\n",
    "        pos_score = get_pos_score(topics['content_ids'], topics['predictions'])\n",
    "        print(f'{id} max positive score is {pos_score}')\n",
    "        scores[id] = pos_score\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=AutoTokenizer.from_pretrained('/mnt/hdd1/wangjingqi/ck/lecr/ft/pmmb2_TiDeTe/26250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=AutoTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [0, 33600, 1238, 177, 71, 57, 1021, 6488, 11, 6488, 36, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [0, 33600, 1238, 177, 71, 57, 1021, 6488, 11, 6488, 36, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m('hell fgd safawerawer o'),m1('hell fgd safawerawer o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.load_state_dict(m.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck = torch.load('/mnt/hdd1/wangjingqi/ck/lecr/xlm-roberta-base_train_50_pmmb2_TiDeTe_26250_TiDeTe_TiDeTe_split32_54_super/xlm-roberta-base_train_50_pmmb2_TiDeTe_26250_TiDeTe_TiDeTe_split32_54_super_0.ckpt',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for XLMRobertaModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.1.output.LayerNorm.weight\", \"encoder.layer.1.output.LayerNorm.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.2.output.LayerNorm.weight\", \"encoder.layer.2.output.LayerNorm.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.3.output.LayerNorm.weight\", \"encoder.layer.3.output.LayerNorm.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.4.output.LayerNorm.weight\", \"encoder.layer.4.output.LayerNorm.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.5.output.LayerNorm.weight\", \"encoder.layer.5.output.LayerNorm.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.6.output.LayerNorm.weight\", \"encoder.layer.6.output.LayerNorm.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.7.output.LayerNorm.weight\", \"encoder.layer.7.output.LayerNorm.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.8.output.LayerNorm.weight\", \"encoder.layer.8.output.LayerNorm.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.9.output.LayerNorm.weight\", \"encoder.layer.9.output.LayerNorm.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.10.output.LayerNorm.weight\", \"encoder.layer.10.output.LayerNorm.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.11.output.LayerNorm.weight\", \"encoder.layer.11.output.LayerNorm.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"global_step\", \"pytorch-lightning_version\", \"state_dict\", \"loops\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m\u001b[39m.\u001b[39;49mload_state_dict(ck)\n",
      "File \u001b[0;32m/mnt/hdd1/wangjingqi/anaconda3/envs/rapids/lib/python3.8/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for XLMRobertaModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.1.output.LayerNorm.weight\", \"encoder.layer.1.output.LayerNorm.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.2.output.LayerNorm.weight\", \"encoder.layer.2.output.LayerNorm.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.3.output.LayerNorm.weight\", \"encoder.layer.3.output.LayerNorm.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.4.output.LayerNorm.weight\", \"encoder.layer.4.output.LayerNorm.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.5.output.LayerNorm.weight\", \"encoder.layer.5.output.LayerNorm.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.6.output.LayerNorm.weight\", \"encoder.layer.6.output.LayerNorm.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.7.output.LayerNorm.weight\", \"encoder.layer.7.output.LayerNorm.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.8.output.LayerNorm.weight\", \"encoder.layer.8.output.LayerNorm.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.9.output.LayerNorm.weight\", \"encoder.layer.9.output.LayerNorm.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.10.output.LayerNorm.weight\", \"encoder.layer.10.output.LayerNorm.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.11.output.LayerNorm.weight\", \"encoder.layer.11.output.LayerNorm.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"global_step\", \"pytorch-lightning_version\", \"state_dict\", \"loops\". "
     ]
    }
   ],
   "source": [
    "m.load_state_dict(ck['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    nw = 4\n",
    "    bs = 256\n",
    "    top_n = 50\n",
    "    seed = 42\n",
    "    device = 0\n",
    "    max_len = 50\n",
    "    uns_key = \"TiDeTe\"\n",
    "    sup_key = \"TiDe\"\n",
    "    model_name = \"/mnt/hdd1/wangjingqi/ck/lecr/ft/pmmb2_TiDeTe/26250\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "topics.shape: (76972, 6)\n",
      "content.shape: (154047, 5)\n",
      "correlations.shape: (61517, 2)\n"
     ]
    }
   ],
   "source": [
    "topics, content, correlations = read_data(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 301/301 [01:45<00:00,  2.85it/s]\n",
      "100%|██████████| 602/602 [07:03<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Training KNN model...\n"
     ]
    }
   ],
   "source": [
    "topics, content = get_neighbors(topics, content, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>parent</th>\n",
       "      <th>Ti</th>\n",
       "      <th>TiDe</th>\n",
       "      <th>TiDeTe</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_b908fd457c9b</td>\n",
       "      <td>es</td>\n",
       "      <td>t_5780107c8277</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>c_a4ab048221a9 c_05b1c711b712 c_a3959a8d38c6 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_975bc0c269f5</td>\n",
       "      <td>en</td>\n",
       "      <td>t_11e7bc1103df</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>c_808b3d694d7e c_f015cfe3c1c5 c_3a093d5f3553 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_9d031273c9c4</td>\n",
       "      <td>zh</td>\n",
       "      <td>t_bd070b953e5c</td>\n",
       "      <td>税</td>\n",
       "      <td>税</td>\n",
       "      <td>税</td>\n",
       "      <td>c_3fd3364c7b0a c_a5f02f4f09a2 c_cef911d05090 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_26915e343b70</td>\n",
       "      <td>zh</td>\n",
       "      <td>t_6fea62eb7a7e</td>\n",
       "      <td>简介</td>\n",
       "      <td>简介</td>\n",
       "      <td>简介</td>\n",
       "      <td>c_25f168ffb66a c_a3b815330028 c_f2c99f213884 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_e105123ddb73</td>\n",
       "      <td>zh</td>\n",
       "      <td>t_4d762b5d1165</td>\n",
       "      <td>艺术</td>\n",
       "      <td>艺术</td>\n",
       "      <td>艺术</td>\n",
       "      <td>c_9af796d59f0a c_fa65796bfbb9 c_8fa78ce3962c c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id language          parent  Ti TiDe TiDeTe  \\\n",
       "0  t_b908fd457c9b       es  t_5780107c8277                   \n",
       "1  t_975bc0c269f5       en  t_11e7bc1103df   9   9      9    \n",
       "2  t_9d031273c9c4       zh  t_bd070b953e5c   税   税      税    \n",
       "3  t_26915e343b70       zh  t_6fea62eb7a7e  简介  简介     简介    \n",
       "4  t_e105123ddb73       zh  t_4d762b5d1165  艺术  艺术     艺术    \n",
       "\n",
       "                                         predictions  \n",
       "0  c_a4ab048221a9 c_05b1c711b712 c_a3959a8d38c6 c...  \n",
       "1  c_808b3d694d7e c_f015cfe3c1c5 c_3a093d5f3553 c...  \n",
       "2  c_3fd3364c7b0a c_a5f02f4f09a2 c_cef911d05090 c...  \n",
       "3  c_25f168ffb66a c_a3b815330028 c_f2c99f213884 c...  \n",
       "4  c_9af796d59f0a c_fa65796bfbb9 c_8fa78ce3962c c...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>parent</th>\n",
       "      <th>Ti</th>\n",
       "      <th>TiDe</th>\n",
       "      <th>TiDeTe</th>\n",
       "      <th>predictions</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_26915e343b70</td>\n",
       "      <td>zh</td>\n",
       "      <td>t_6fea62eb7a7e</td>\n",
       "      <td>简介</td>\n",
       "      <td>简介</td>\n",
       "      <td>简介</td>\n",
       "      <td>c_25f168ffb66a c_a3b815330028 c_f2c99f213884 c...</td>\n",
       "      <td>t_26915e343b70</td>\n",
       "      <td>c_03697937b392 c_b380020ac642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_909b58c6d293</td>\n",
       "      <td>zh</td>\n",
       "      <td>t_02402e8102b8</td>\n",
       "      <td>投篮</td>\n",
       "      <td>投篮</td>\n",
       "      <td>投篮</td>\n",
       "      <td>c_0c1899fb281f c_980187e51826 c_60295d0182cd c...</td>\n",
       "      <td>t_909b58c6d293</td>\n",
       "      <td>c_0c1899fb281f c_60295d0182cd c_683d0adcdb39 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_ab4c1b92c735</td>\n",
       "      <td>zh</td>\n",
       "      <td>t_02402e8102b8</td>\n",
       "      <td>防守</td>\n",
       "      <td>防守</td>\n",
       "      <td>防守</td>\n",
       "      <td>c_9f66c1641b50 c_10785744108b c_417058c2b39e c...</td>\n",
       "      <td>t_ab4c1b92c735</td>\n",
       "      <td>c_10785744108b c_2d71d8a7adcc c_417058c2b39e c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_3f13d204a214</td>\n",
       "      <td>zh</td>\n",
       "      <td>t_2d2fafe51d9d</td>\n",
       "      <td>挥杆</td>\n",
       "      <td>挥杆</td>\n",
       "      <td>挥杆</td>\n",
       "      <td>c_8d537721a710 c_2b6f7b40b473 c_5b451d883626 c...</td>\n",
       "      <td>t_3f13d204a214</td>\n",
       "      <td>c_209cf4acb7f9 c_24c7ef246641 c_2b6f7b40b473 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_527deeb1e651</td>\n",
       "      <td>zh</td>\n",
       "      <td>t_696a0f81e676</td>\n",
       "      <td>访谈</td>\n",
       "      <td>访谈</td>\n",
       "      <td>访谈</td>\n",
       "      <td>c_7a04cf6c64b8 c_3278e8557417 c_64ccf43280e0 c...</td>\n",
       "      <td>t_527deeb1e651</td>\n",
       "      <td>c_3278e8557417 c_7a04cf6c64b8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id language          parent  Ti TiDe TiDeTe  \\\n",
       "0  t_26915e343b70       zh  t_6fea62eb7a7e  简介  简介     简介    \n",
       "1  t_909b58c6d293       zh  t_02402e8102b8  投篮  投篮     投篮    \n",
       "2  t_ab4c1b92c735       zh  t_02402e8102b8  防守  防守     防守    \n",
       "3  t_3f13d204a214       zh  t_2d2fafe51d9d  挥杆  挥杆     挥杆    \n",
       "4  t_527deeb1e651       zh  t_696a0f81e676  访谈  访谈     访谈    \n",
       "\n",
       "                                         predictions        topic_id  \\\n",
       "0  c_25f168ffb66a c_a3b815330028 c_f2c99f213884 c...  t_26915e343b70   \n",
       "1  c_0c1899fb281f c_980187e51826 c_60295d0182cd c...  t_909b58c6d293   \n",
       "2  c_9f66c1641b50 c_10785744108b c_417058c2b39e c...  t_ab4c1b92c735   \n",
       "3  c_8d537721a710 c_2b6f7b40b473 c_5b451d883626 c...  t_3f13d204a214   \n",
       "4  c_7a04cf6c64b8 c_3278e8557417 c_64ccf43280e0 c...  t_527deeb1e651   \n",
       "\n",
       "                                         content_ids  \n",
       "0                      c_03697937b392 c_b380020ac642  \n",
       "1  c_0c1899fb281f c_60295d0182cd c_683d0adcdb39 c...  \n",
       "2  c_10785744108b c_2d71d8a7adcc c_417058c2b39e c...  \n",
       "3  c_209cf4acb7f9 c_24c7ef246641 c_2b6f7b40b473 c...  \n",
       "4                      c_3278e8557417 c_7a04cf6c64b8  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = topics.merge(correlations, how = 'inner', left_on = ['id'], right_on = ['topic_id'])\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our max positive score is 0.94418\n"
     ]
    }
   ],
   "source": [
    "pos_score = get_pos_score(topics['content_ids'], topics['predictions'])\n",
    "print(f'Our max positive score is {pos_score}')#(0.94418)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61517/61517 [00:53<00:00, 1146.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 3075850 rows 0.08459417721930523\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "# Set id as index for content\n",
    "# Build training set\n",
    "train = build_training_set(topics, content, CFG)\n",
    "print(f'Our training set has {len(train)} rows',train.target.sum()/len(train))\n",
    "# Save train set to disk to train on another notebook\n",
    "#0.82,0.310,0.92365,0.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"_\".join(CFG.model_name.split('/')[-2:])\n",
    "\n",
    "train.to_csv(f'/mnt/hdd1/wangjingqi/dataset/lecr/train_{CFG.top_n}_{model_name}_{CFG.uns_key}_{CFG.sup_key}_super.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3095572, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"/mnt/hdd1/wangjingqi/dataset/lecr/train_50_pmmb2_TiDeTe_26250_TiDeTe_TiDe.csv\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fe4e0406d3ad7106b208933fab2b0f2ca15cbb9618f84ec8133c53d1894d1ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
