{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T04:39:24.343384Z","iopub.status.busy":"2022-11-18T04:39:24.343081Z","iopub.status.idle":"2022-11-18T04:39:29.710927Z","shell.execute_reply":"2022-11-18T04:39:29.709954Z","shell.execute_reply.started":"2022-11-18T04:39:24.343314Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["namespace(accumulate_grad_batches=None, bert='microsoft/deberta-v3-large', bs=4, ck='/home/wangjingqi/input/ck/fpell', device_ids=[7], epochs=5, freeze=None, head_lr=0.0001, layer_start=-1, llrd=0.4, llrd_interval=1, log='/home/wangjingqi/fpell-pl/log', max_len=640, model_fname='deberta-v3-large_lr1-wd-0.4-0.001-1-1-1-640-5-2022', nfolds=5, patience=10, precision=16, prefix='lr1-wd', reinit_layers=1, seed=2022, submit='/home/wangjingqi/input/dataset/fpell/sample_submission.csv', test='/home/wangjingqi/input/dataset/fpell/test.csv', train='/home/wangjingqi/input/dataset/fpell/train/train.csv', used_folds=[0, 1, 2, 3, 4], val_check_interval=1.0, wd=0.001)\n"]}],"source":["import sys\n","sys.path.append(\"../input/iterstrat\")\n","import pandas as pd\n","import re\n","import string\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","from transformers import AutoModel,AutoTokenizer\n","import torch\n","import torch.nn.functional as F\n","from sklearn.linear_model import Ridge\n","from sklearn.multioutput import MultiOutputRegressor\n","import lightgbm as lgb\n","from tqdm import tqdm\n","import numpy as np\n","import warnings\n","from config import config\n","\n","warnings.filterwarnings(\"ignore\")                                                   "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["196.0"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["4900/25"]},{"cell_type":"markdown","metadata":{},"source":["**Load data**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T04:39:29.722057Z","iopub.status.busy":"2022-11-18T04:39:29.721425Z","iopub.status.idle":"2022-11-18T04:39:29.992248Z","shell.execute_reply":"2022-11-18T04:39:29.986677Z","shell.execute_reply.started":"2022-11-18T04:39:29.722022Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(3911, 13)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text_id</th>\n","      <th>full_text</th>\n","      <th>cohesion</th>\n","      <th>syntax</th>\n","      <th>vocabulary</th>\n","      <th>phraseology</th>\n","      <th>grammar</th>\n","      <th>conventions</th>\n","      <th>topic</th>\n","      <th>topic_prob</th>\n","      <th>topic_name</th>\n","      <th>src</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0016926B079C</td>\n","      <td>I think that students would benefit from learn...</td>\n","      <td>3.5</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>students online school classes</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0022683E9EA5</td>\n","      <td>When a problem is a change you have to let it ...</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>advice people ask multiple</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>00299B378633</td>\n","      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n","      <td>3.0</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.5</td>\n","      <td>10</td>\n","      <td>1.000000</td>\n","      <td>sports average school students</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>003885A45F42</td>\n","      <td>The best time in life is when you become yours...</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>advice people ask multiple</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0049B1DF5CCC</td>\n","      <td>Small act of kindness can impact in other peop...</td>\n","      <td>2.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","      <td>12</td>\n","      <td>0.715767</td>\n","      <td>community service community service help</td>\n","      <td>train</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0       text_id  \\\n","0           0  0016926B079C   \n","1           1  0022683E9EA5   \n","2           2  00299B378633   \n","3           3  003885A45F42   \n","4           4  0049B1DF5CCC   \n","\n","                                           full_text  cohesion  syntax  \\\n","0  I think that students would benefit from learn...       3.5     3.5   \n","1  When a problem is a change you have to let it ...       2.5     2.5   \n","2  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5   \n","3  The best time in life is when you become yours...       4.5     4.5   \n","4  Small act of kindness can impact in other peop...       2.5     3.0   \n","\n","   vocabulary  phraseology  grammar  conventions  topic  topic_prob  \\\n","0         3.0          3.0      4.0          3.0      2    1.000000   \n","1         3.0          2.0      2.0          2.5      1    1.000000   \n","2         3.0          3.0      3.0          2.5     10    1.000000   \n","3         4.5          4.5      4.0          5.0      1    1.000000   \n","4         3.0          3.0      2.5          2.5     12    0.715767   \n","\n","                                 topic_name    src  \n","0            students online school classes  train  \n","1                advice people ask multiple  train  \n","2            sports average school students  train  \n","3                advice people ask multiple  train  \n","4  community service community service help  train  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(config.train)\n","df[\"src\"] = \"train\"\n","ss = pd.read_csv(config.submit)\n","target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions',]\n","print(df.shape)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["**MultilabelStratifiedKFold**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T04:39:30.230068Z","iopub.status.busy":"2022-11-18T04:39:30.229736Z","iopub.status.idle":"2022-11-18T04:39:30.242056Z","shell.execute_reply":"2022-11-18T04:39:30.241107Z","shell.execute_reply.started":"2022-11-18T04:39:30.230036Z"},"trusted":true},"outputs":[],"source":["class fpe_dataset(torch.utils.data.Dataset):\n","    def __init__(self,df):\n","        self.df = df.reset_index(drop=True)\n","    def __len__(self):\n","        return len(self.df)\n","    def __getitem__(self,idx):\n","        text = self.df.loc[idx,\"full_text\"]\n","\n","        return text\n","class collator():\n","    def __init__(self,max_len,pretrained_path,) -> None:\n","        self.max_len = max_len\n","        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n","    def __call__(self, data):\n","            \n","        X = self.tokenizer(\n","                data,\n","                None,max_length=self.max_len,\n","                padding='max_length',truncation=True,return_tensors=\"pt\")\n","        return X\n","\n","def preprocess(text):\n","    text = text.split(\" \")\n","    while 2>1:\n","        if text[-1] in [\"\",\" \",\"\\n\",\"\\r\"]:\n","            text.pop(-1)\n","        else:\n","            break\n","    return \" \".join(text)\n","df['full_text'] = df['full_text'].apply(preprocess)"]},{"cell_type":"markdown","metadata":{},"source":["**Model**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T04:39:30.260965Z","iopub.status.busy":"2022-11-18T04:39:30.260291Z","iopub.status.idle":"2022-11-18T04:39:36.512679Z","shell.execute_reply":"2022-11-18T04:39:36.511628Z","shell.execute_reply.started":"2022-11-18T04:39:30.260932Z"},"trusted":true},"outputs":[],"source":["\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import AutoModel,AutoTokenizer\n","import pytorch_lightning as pl\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","from transformers import get_cosine_schedule_with_warmup,AdamW,get_linear_schedule_with_warmup\n","from copy import deepcopy\n","import math\n","\n","\n","class MaxPooling(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","    def forward(self,hidden_state,attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n","        hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n","        max_embeddings = torch.max(hidden_state, 1)[0]\n","        return max_embeddings\n","\n","class MeanPooling(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","    def forward(self,hidden_state, attention_mask):\n","\n","        input_mask_expanded = (\n","            attention_mask.unsqueeze(-1).expand(hidden_state.size())\n","        )\n","        mean_embeddings = torch.sum(hidden_state * input_mask_expanded, 1) / torch.clamp(\n","            input_mask_expanded.sum(1), min=1e-9)\n","        return mean_embeddings\n","\n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start, layer_weights = None):\n","        super().__init__()\n","        if layer_start < 0:\n","            self.layer_start =num_hidden_layers+1+layer_start\n","        else:\n","            self.layer_start = layer_start\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","                torch.tensor([1] * (num_hidden_layers+1 - self.layer_start), dtype=torch.float)\n","            )\n","\n","    def forward(self, features):\n","\n","        all_layer_embedding = torch.stack(features)\n","        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n","\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","\n","        return weighted_average\n","\n","\n","\n","class deberta(nn.Module):\n","    def __init__(self,config) -> None:\n","        super().__init__()\n","        self.config = config\n","        self.encoder = AutoModel.from_pretrained(config.bert,attention_probs_dropout_prob=0.,hidden_dropout_prob=0.,output_hidden_states=True)\n","        hidden_size = self.encoder.config.hidden_size\n","        num_attention_heads = self.encoder.config.num_attention_heads\n","        num_hidden_layers =  self.encoder.config.num_hidden_layers\n","        \n","        self.WeightedLayerPooling =nn.Sequential(WeightedLayerPooling(num_hidden_layers=num_hidden_layers,layer_start=config.layer_start,))\n","        self.Pooler = MeanPooling()\n","        self.head =nn.Sequential(nn.Linear(hidden_size,hidden_size),nn.GELU(),nn.Linear(hidden_size,6))\n","    def forward(self,input_ids,attention_mask,token_type_ids):\n","\n","        b,l = input_ids.shape\n","        x = dict(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n","        #for deberta\n","        encoder = self.encoder(**x)\n","        all_hidden_states =encoder.hidden_states\n","        hidden_states = self.WeightedLayerPooling(all_hidden_states)\n","        hidden_states = self.Pooler(hidden_states,attention_mask)#bs,768\n","        return hidden_states\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T04:39:36.514737Z","iopub.status.busy":"2022-11-18T04:39:36.514053Z","iopub.status.idle":"2022-11-18T04:39:36.529534Z","shell.execute_reply":"2022-11-18T04:39:36.527900Z","shell.execute_reply.started":"2022-11-18T04:39:36.514706Z"},"trusted":true},"outputs":[],"source":["\n","import os\n","from torch.utils.data import DataLoader\n","def get_embeddings(mp='',df=None,verbose=True):\n","\n","    ck = torch.load(mp,map_location=torch.device('cpu'))\n","    model_config = ck[\"config\"]\n","    # pretrained = os.path.join(\"../input/deberta\",os.path.basename(model_config.bert))\n","    # model_config.bert = os.path.join(pretrained,\"model\")\n","    model_config.tokenizer=model_config.bert#os.path.join(pretrained,\"tokenizer\")\n","    model=deberta(model_config)\n","    model.load_state_dict(state_dict=ck[\"model\"]) \n","    \n","    dataset = fpe_dataset(df)\n","    dataloader = DataLoader(dataset,batch_size =32, shuffle = False, pin_memory=True,collate_fn = collator(max_len=model_config.max_len,pretrained_path=model_config.tokenizer),drop_last=False)\n","    model.cuda()\n","    model.eval()\n","    model.float()\n","\n","    all_text_feats = []\n","    for batch in tqdm(dataloader,total=len(dataloader)):\n","        input_ids = batch[\"input_ids\"].cuda()\n","        attention_mask = batch[\"attention_mask\"].cuda()\n","        token_type_ids = batch[\"token_type_ids\"].cuda()\n","        with torch.no_grad():\n","            model_output = model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n","        # Normalize the embeddings\n","        sentence_embeddings = F.normalize(model_output, p=2, dim=1)\n","        sentence_embeddings =  sentence_embeddings.detach().cpu().numpy()\n","        all_text_feats.extend(sentence_embeddings)\n","    all_text_feats = np.array(all_text_feats)\n","    if verbose:\n","        print('Train embeddings shape',all_text_feats.shape)\n","        \n","    return all_text_feats"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T04:39:36.531306Z","iopub.status.busy":"2022-11-18T04:39:36.530879Z","iopub.status.idle":"2022-11-18T04:39:36.542965Z","shell.execute_reply":"2022-11-18T04:39:36.541977Z","shell.execute_reply.started":"2022-11-18T04:39:36.531271Z"},"trusted":true},"outputs":[],"source":["all_feats = []"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T04:39:36.544504Z","iopub.status.busy":"2022-11-18T04:39:36.544233Z","iopub.status.idle":"2022-11-18T04:42:50.174687Z","shell.execute_reply":"2022-11-18T04:42:50.172325Z","shell.execute_reply.started":"2022-11-18T04:39:36.544480Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["10 ['/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022_2.ckpt', '/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022_4.ckpt', '/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022_0.ckpt', '/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022_1.ckpt', '/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022_3.ckpt', '/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022_0.ckpt', '/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022_4.ckpt', '/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022_2.ckpt', '/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022_1.ckpt', '/home/wangjingqi/input/ck/fpell/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022/deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022_3.ckpt']\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 123/123 [06:23<00:00,  3.11s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Train embeddings shape (3911, 768)\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 123/123 [06:20<00:00,  3.10s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Train embeddings shape (3911, 768)\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 123/123 [06:21<00:00,  3.10s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Train embeddings shape (3911, 768)\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"," 86%|████████▌ | 106/123 [05:30<00:52,  3.07s/it]"]}],"source":["def get_path(m_dirs):\n","    model_list = []\n","    for m in m_dirs:\n","        model_list +=[os.path.join(m,i) for i in os.listdir(m)]\n","    return model_list\n","\n","m_dirs = [\"deberta-v3-base_lr1-wd-0.4-0-1-1-1-640-5-2022\",\n","\"deberta-v3-base_lr1-wd-0.4-0-1-1-1-1024-5-2022\",]\n","m_dirs = [os.path.join(\"\",i) for i in m_dirs]\n","model_path =get_path(m_dirs)\n","print(len(model_path),model_path)\n","for m in model_path:\n","    text_feats = get_embeddings(m,df)\n","    all_feats.append(text_feats)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T04:42:50.187342Z","iopub.status.busy":"2022-11-18T04:42:50.186953Z","iopub.status.idle":"2022-11-18T04:42:50.204799Z","shell.execute_reply":"2022-11-18T04:42:50.203723Z","shell.execute_reply.started":"2022-11-18T04:42:50.187304Z"},"trusted":true},"outputs":[],"source":["all_feats = np.concatenate(all_feats,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T04:42:50.206785Z","iopub.status.busy":"2022-11-18T04:42:50.206370Z","iopub.status.idle":"2022-11-18T04:42:50.214562Z","shell.execute_reply":"2022-11-18T04:42:50.213455Z","shell.execute_reply.started":"2022-11-18T04:42:50.206749Z"},"trusted":true},"outputs":[],"source":["print('Our concatenated embeddings have shape', all_feats.shape )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.save(\"/home/wangjingqi/input/ck/fpell/all_feats.npy\",all_feats)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('game': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.15"},"vscode":{"interpreter":{"hash":"9cd4817dc6c5c8851b0d2b0917103f420c02e169bb3e664c040bdb378fb18d97"}}},"nbformat":4,"nbformat_minor":4}
